# WikiAnalysis: Data Collection and Processing

This repository hosts the Python scripts utilized in the data collection and processing stages of our research projects that explore the bias in Wikipedia's notability guidelines and their implications. 

The repository brings together scripts from three folders: "g_search", "WikiDataPull", and "wikidata_info". Each of these folders contributes to a different segment of our data analysis pipeline, involving the collection of data on academics whose Wikipedia articles have been flagged for deletion and subsequent data processing.

## Repository Structure

The repository comprises three main folders:

1. **WikiDataPull**: This folder contains the Python script `wikiprojscrapingcode.py` that was used to scrape data from a specific Wikipedia page: [Wikipedia: WikiProject Deletion sorting/Academics and educators/archive_2](https://en.m.wikipedia.org/wiki/Wikipedia:WikiProject_Deletion_sorting/Academics_and_educators/archive_2). This page serves as an archive of academics and educators whose articles were flagged for deletion. The output is a .csv file named `wikicombo.csv` with the academic names and their respective Wikipedia links.

2. **wikidata_info**: This folder hosts scripts used to gather additional information on all academics whose Wikipedia articles have been flagged for deletion from 2017-2020. This data collection is built upon the `wikicombo.csv` file generated by the `wikiprojscrapingcode.py` script.

3. **g_search**: The scripts in this folder use the information collected from the previous folders and additional data collected from our webscraping process to generate a Google Index. This index is then compared to a corresponding Primer Index as discussed in our publications.

## Disclaimer

Please note that the specific Wikipedia page we scraped continually updates with more "Articles for Deletion" (AfD) discussions over time. Therefore, our .csv file represents a snapshot of the deletion discussions available from around the summer of 2020, dating back to sometime in 2017.

Feel free to explore the code and the data output to better understand how our research projects collected and processed the data.

